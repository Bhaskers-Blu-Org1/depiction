{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depiction - A meta-interpretability toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why interpretability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last decade, the application of deep neural networks to long-standing problems has brought a break-through in performance and prediction power. However, high accuracy, deriving from the increased model complexity, often comes at the price of loss of interpretability, i.e., many of these models behave as black-boxes and fail to provide explanations on their predictions. While in certain application fields this issue may play a secondary role, in high risk domains, e.g., health care, it is crucial to build trust in a model and being able to understand its behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is interpretability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of the verb *interpret* is \"to explain or tell the meaning of : present in understandable terms\" ([Merriam-Webster 2019](https://www.merriam-webster.com/dictionary/interpret)). Despite the apparent simplicity of this statement, the machine learning research community is struggling to agree upon a formal definition of the concept of interpretability/explainability. In the last years, in the room left by this lack of formalism, many methodologies have been proposed based on different \"interpretations\" (pun intended) of the above defintion. While the proliferation of this multitude of disparate algorithms has posed challenges on rigorously comparing them, it is nevertheless interesting and useful to apply these techniques to analyze the behaviour of deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this tutorial about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial focuses on illustrating some of the recent advancements in the field of interpretable deep learning. We will show common techniques that can be used to explain predictions on pretrained models and that can be used to shed light on their inner mechanisms. The tutorial is aimed to strike the right balance between theoretical input and practical exercises. The session has been designed to provide the participants not only with the theory behind deep learning interpretability, but also to offer a set of frameworks and tools that they can easily reuse in their own projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The group of Cognitive Health Care and Life Sciences at IBM Research ZÃ¼rich has opensourced a python toolbox, [depiction](https://github.com/IBM/depiction), with the aim of providing a framework to ease the application of explainability methods on custom models, especially for less experienced users. The module provide wrappers for multiple algorithms and is continously updated including the latest algorithms from [AIX360](https://github.com/IBM/AIX360.git). The core concept behind depiction is to allow users to seamlessly run state-of-art interpretability methods with minimal requirements in terms of programming skills. Below an example of how depiction can be used to analyze a pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example - Wrapping a pretrained Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume to have a fancy model for classification of tabular data pretrained in Keras and available at a public url. Explaining its predictions with `depiction` is easy as implementing a lightweight wrapper of `depiction.models.uri.HTTPModel` where its `predict` method is overridden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from depiction.core import Task, DataType\n",
    "from depiction.models.uri import HTTPModel\n",
    "\n",
    "\n",
    "class FancyModel(HTTPModel):\n",
    "    \"\"\"A fancy classifier.\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "        filename='fancy_model.h5',\n",
    "        origin='https://url/to/my/fancy_model.h5',\n",
    "        cache_dir='/path/to/cache/models',\n",
    "        *args, **kwargs):\n",
    "        \"\"\"Initialize the FancyModel.\"\"\"\n",
    "        super().__init__(\n",
    "            uri=origin,\n",
    "            task=Task.CLASSIFICATION,\n",
    "            data_type=DataType.TABULAR,\n",
    "            cache_dir=cache_dir,\n",
    "            filename=filename\n",
    "        )\n",
    "        self.model = keras.models.load_model(self.model_path)\n",
    "\n",
    "    def predict(self, sample, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Run the fancy model for inference on a given sample and with the provided\n",
    "        parameters.\n",
    "\n",
    "        Args:\n",
    "            sample (object): an input sample for the model.\n",
    "            args (list): list of arguments.\n",
    "            kwargs (dict): list of key-value arguments.\n",
    "\n",
    "        Returns:\n",
    "            a prediction for the model on the given sample.\n",
    "        \"\"\"\n",
    "        return self.model.predict(\n",
    "            sample,\n",
    "            batch_size=None, verbose=0,\n",
    "            steps=None, callbacks=None\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once `FancyModel` is implemented, using any of the `depiction.interpreters` available in the library, is as easy as typing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "fancy_model = FancyModel()\n",
    "# NOTE: interpreters are implemented inheriting from\n",
    "# depiction.interpreters.base.base_interpreter.BaseInterpreter\n",
    "# and they share a common interface.\n",
    "explanations = interpreter.interpret(example)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another simple example - Rule-based explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
